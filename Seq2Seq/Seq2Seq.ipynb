{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "Seq2Seq.ipynb",
      "provenance": [],
      "collapsed_sections": [],
      "toc_visible": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "accelerator": "GPU"
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "8-qMB8tC6JEz"
      },
      "source": [
        "## Notebook for running Seq2Seq model in Google Colab"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "KPbWPJbF6WQr"
      },
      "source": [
        "Let's import all the code for the task of Seq2Seq neural machine translation and run it on Colab."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "vBSf28Cs3Vjk",
        "outputId": "0d025911-5687-4fd2-847f-701e8970ef4a"
      },
      "source": [
        "from google.colab import drive\n",
        "drive.mount('/content/drive')"
      ],
      "execution_count": 1,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Mounted at /content/drive\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "I_PrD5jx3Wq-"
      },
      "source": [
        "!cp -r '/content/drive/MyDrive/Colab Notebooks/NLP1C/Seq2Seq/' ."
      ],
      "execution_count": 2,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "isfzRdGi5K-2"
      },
      "source": [
        "import os"
      ],
      "execution_count": 13,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "17PMcqfQ7rAN"
      },
      "source": [
        "### Training the model"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "UM_4pSi27Lss"
      },
      "source": [
        "Let's create a necessary vocab file and start training the model."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "pu37kq0t5NW1"
      },
      "source": [
        "os.chdir('Seq2Seq')"
      ],
      "execution_count": 8,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "LhkZloto3kuF",
        "outputId": "3afa82cf-3b44-4e47-8048-0396a9455b16"
      },
      "source": [
        "!sh run.sh vocab"
      ],
      "execution_count": 11,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "[nltk_data] Downloading package punkt to /root/nltk_data...\n",
            "[nltk_data]   Package punkt is already up-to-date!\n",
            "read in source sentences: ./en_es_data/train.es\n",
            "read in target sentences: ./en_es_data/train.en\n",
            "initialize source vocabulary ..\n",
            "number of word types: 93286, number of word types w/ frequency >= 2: 52658\n",
            "initialize target vocabulary ..\n",
            "number of word types: 67535, number of word types w/ frequency >= 2: 39786\n",
            "generated vocabulary, source 50004 words, target 39788 words\n",
            "vocabulary saved to vocab.json\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "oq2fEvT25IjY"
      },
      "source": [
        "!sh run.sh train"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "NPMGkIme7uzi"
      },
      "source": [
        "### Testing the model"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "KCRjapIy6COX"
      },
      "source": [
        "Now let's test the model."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "Dlhp-ip47i6C",
        "outputId": "7205b4fc-f0d5-4cc0-c118-c55174c829ba"
      },
      "source": [
        "!sh run.sh test"
      ],
      "execution_count": 27,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "[nltk_data] Downloading package punkt to /root/nltk_data...\n",
            "[nltk_data]   Package punkt is already up-to-date!\n",
            "load test source sentences from [./en_es_data/test.es]\n",
            "load test target sentences from [./en_es_data/test.en]\n",
            "load model from model.bin\n",
            "Decoding:   0% 0/8064 [00:00<?, ?it/s]\n",
            "Traceback (most recent call last):\n",
            "  File \"run.py\", line 344, in <module>\n",
            "    main()\n",
            "  File \"run.py\", line 338, in main\n",
            "    decode(args)\n",
            "  File \"run.py\", line 283, in decode\n",
            "    max_decoding_time_step=int(args['--max-decoding-time-step']))\n",
            "  File \"run.py\", line 311, in beam_search\n",
            "    example_hyps = model.beam_search(src_sent, beam_size=beam_size, max_decoding_time_step=max_decoding_time_step)\n",
            "  File \"/content/Seq2Seq/nmt_model.py\", line 454, in beam_search\n",
            "    new_hyp_sent = hypotheses[prev_hyp_id] + [hyp_word]\n",
            "TypeError: list indices must be integers or slices, not float\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "sdb2YUF776pv"
      },
      "source": [
        "### Answering the written questions"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "BH8jKl037_Ix"
      },
      "source": [
        "**(g)** The `generate_sent_masks()` function in `nmt_model.py` produces a tensor called `enc_masks`. It has shape (batch size, max source sentence length) and contains 1s in positions corresponding to 'pad' tokens in the input, and 0s for non-pad tokens. Look at how the masks are used during the attention computation in the `step()` function.\n",
        "<br>\n",
        "First explain (in around three sentences) what effect the masks have on the entire attention computation. Then explain (in one or two sentences) why it is necessary to use the masks in this way. \n",
        "<br><br>\n",
        "**Solution.**\n",
        "<br>\n",
        " By looking at the `step()` function we see that all the attention scores, which have 1s in their mask (if it is a 'pad' token) are set to `-float(inf)`. As a result, these tokens get a very small weight in probability distridution after applying `softmax`. Therefore, it is necessary to use masks in this way, because without applying this technique the 'pad' tokens might transform the real attention distribution.\n",
        "<br><br>\n",
        "**(i)** Please report the model's corpus BLEU Score. It should be larger than 21.\n",
        "<br><br>\n",
        "**Solution.**\n",
        "<br>\n",
        "*Early stopped model:* <br>\n",
        "training: epoch 15, iter 96000, avg. loss 24.51, avg. ppl 3.25, <br>\n",
        "validation: iter 96000, dev. ppl 7.318153.\n",
        "<br>\n",
        "Model's corpus BLEU score: \n",
        "<br><br>\n",
        "**(j)** In class, we learned about dot product attention, multiplicative attention, and additive attention. Please explain one advantage and one disadvantage of dot product attention compared to multiplicative attention. Then explain one advantage and one disadvantage of additive attention compared to multiplicative attention. As a reminder, dot product attention is $\\mathbf{e}_{t, i} = \\mathbf{s}^{T} \\mathbf{h}_{i}$, multiplicative attention is $\\mathbf{e}_{t, i} = \\mathbf{s}_{t}^{T} \\mathbf{W} \\mathbf{h}_{i}$, and additive attention is $\\mathbf{e}_{t, i} = \\mathbf{v}^{T} \\tanh{(\\mathbf{W}_{1} \\mathbf{h}_{i} + \\mathbf{W}_{2} \\mathbf{s}_{t})}$.\n",
        "<br><br>\n",
        "**Solution.**\n",
        "<br>\n",
        "*Comparing dot product and multiplicative attention:* dot product attention is faster and more memory efficient, however it is not as flexible as multiplicative attention.\n",
        "<br>\n",
        "*Comparing multiplicative and additive attention:* multiplicative attention is faster and more memory efficient, however additive attention performs better for larger dimensions and more flexible because both source and target hidden state vectors have their own learnable matrices $\\mathbf{W}_{1}$ and $\\mathbf{W}_{2}$.\n",
        "\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "gB3IKZLBAHiv"
      },
      "source": [
        ""
      ],
      "execution_count": null,
      "outputs": []
    }
  ]
}